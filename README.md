# Enhancing Traceability in Software Engineering: A Fine-Tuned Language Model Approach

## Authors
- **John Melwin Richard**
  - Master of Science, Data Science
  - Rochester Institute of Technology
  - Email: jj5603@rit.edu
- **Zhe Yu**
  - Assistant Professor of Software Engineering
  - Rochester Institute of Technology
  - Email: zxyvse@rit.edu

## Goals of the Paper
This research introduces a novel approach using transformer-based models to enhance traceability in software development. It aims to compare this method with existing techniques and explore the transition from traditional to transformer-based traceability in software engineering.

## Background
Software traceability involves establishing links between different parts of software development like requirements, source code, and test cases. Traditional methods often rely on manual effort, which becomes inefficient for large projects. The use of advanced transformer-based models offers a promising solution to automate and improve this process.

## Scientific Merit
The study explores using transformer-based models for automated identification of links within code artifacts. It seeks to address several key questions, such as the feasibility of enhancing traceability using these models, comparing them with existing techniques, and understanding the transition to transformer-based traceability.

## Broader Impact
The implementation of transformer-based models in software traceability can significantly benefit sponsoring organizations, developers, and the academic community. It can lead to more efficient software development processes, reduced errors, and innovative solutions in related domains.

## Goals and Tasks
- Comprehensive literature review to identify gaps in software traceability methods.
- Implementation of various language models, focusing on CodeT5.
- In-depth exploration of the training process and model architecture.
- Collection and preparation of diverse datasets for software traceability.
- Model fitting, fine-tuning, testing, and performance analysis.

## Literature Review
The review covers various aspects of software traceability, including initial approaches, impacts, machine learning techniques, and advancements in neural networks and transformer-based models like CodeT5 and CodeT5+.

## Approach
The study uses pre-trained Large Language Models (LLMs) for identifying trace links in software. It includes dataset preparation, model selection, fine-tuning, and evaluation using metrics like precision, recall, and F1 score.

## Architecture
The model architecture involves systematic data organization, using the Hugging Face's Transformers library, and a sequential training approach. It includes data preprocessing, fine-tuning, and evaluation stages.

## Implementation Overview
Key points include structured implementation using CodeT5+, efficient data organization, and a focus on reproducibility and ethical AI principles.

## Privacy and Ethical Statement
The research ensures ethical considerations and privacy concerns, using non-proprietary and publicly accessible data, and adhering to responsible technology use principles.

## Dataset
Access the dataset used for this research [here](https://github.com/john
