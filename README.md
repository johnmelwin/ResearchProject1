
# Enhancing Traceability in Software Engineering: A Fine-Tuned Language Model Approach ğŸš€
(Kindly visit the folders for specific README files)
## Authors ğŸ“
- **John Melwin Richard**
  - ğŸ“ Master of Science, Data Science
  - ğŸ« Rochester Institute of Technology
  - ğŸ“§ Email: jj5603@rit.edu
- **Zhe Yu**
  - ğŸ“ Assistant Professor of Software Engineering
  - ğŸ« Rochester Institute of Technology
  - ğŸ“§ Email: zxyvse@rit.edu

## Goals of the Paper ğŸ¯
This research introduces a novel approach using transformer-based models to enhance traceability in software development. It aims to compare this method with existing techniques and explore the transition from traditional to transformer-based traceability in software engineering.

## Scientific Merit ğŸ”¬
The study explores using transformer-based models for automated identification of links within code artifacts. It seeks to address several key questions, such as the feasibility of enhancing traceability using these models, comparing them with existing techniques, and understanding the transition to transformer-based traceability. The implementation of transformer-based models in software traceability can significantly benefit sponsoring organizations, developers, and the academic community. It can lead ...

## Goals and Tasks ğŸ“Œ
- ğŸ” Comprehensive literature review to identify gaps in software traceability methods.
- ğŸ’» Implementation of various language models, focusing on CodeT5.
- ğŸ§  In-depth exploration of the training process and model architecture.
- ğŸ“Š Collection and preparation of diverse datasets for software traceability.
- ğŸ› ï¸ Model fitting, fine-tuning, testing, and performance analysis.

## Approach ğŸ›¤ï¸
The study uses pre-trained Large Language Models (LLMs) for identifying trace links in software. It includes dataset preparation, model selection, fine-tuning, and evaluation using metrics like precision, recall, and F1 score. The model architecture involves systematic data organization, using the Hugging Face's Transformers library, and a sequential training approach. It includes data preprocessing, fine-tuning, and evaluation stages.

## Privacy and Ethical Statement ğŸ›¡ï¸
The research ensures ethical considerations and privacy concerns, using non-proprietary and publicly accessible data, and adhering to responsible technology use principles.

## Dataset ğŸ“‚
Access the dataset used for this research [here](https://github.com/johnmelwin/ResearchProject1/tree/main/Data).
