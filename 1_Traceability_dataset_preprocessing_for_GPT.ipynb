{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBBUALd420KAuubjV+QUM0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnmelwin/ResearchProject1/blob/main/1_Traceability_dataset_preprocessing_for_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PreProcess the data**"
      ],
      "metadata": {
        "id": "1Iw_HBu60Ozw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "FNxLrefSrGWx",
        "outputId": "f305e682-5dfe-483b-98ac-72d13c455d9e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-4ebacb6e9938>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmethod_implementation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmethods_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"methodid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mmethod_implementation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"method\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Define the paths to the JSON files\n",
        "traces_file_path = '/content/traces.json'\n",
        "methods_file_path = '/content/methods.json'\n",
        "\n",
        "# Load the traces.json file\n",
        "with open(traces_file_path, 'r') as file:\n",
        "    traces_data = json.load(file)\n",
        "\n",
        "# Load the methods.json file\n",
        "with open(methods_file_path, 'r') as file:\n",
        "    methods_data = json.load(file)\n",
        "\n",
        "# Initialize a list to hold the content for the JSONL output\n",
        "jsonl_content = []\n",
        "\n",
        "# Initialize counters for each goldfinal value\n",
        "count_T = 0\n",
        "count_E = 0\n",
        "count_N = 0\n",
        "\n",
        "# Iterate through each trace in traces_data\n",
        "for trace in traces_data:\n",
        "    # Find the corresponding method implementation using the method id\n",
        "    method_implementation = None\n",
        "    for method in methods_data:\n",
        "        if method.get(\"id\") == trace[\"methodid\"]:\n",
        "            method_implementation = method.get(\"method\")\n",
        "            break\n",
        "\n",
        "    # Determine the traceability status and update counts based on the 'goldfinal' value\n",
        "    if trace[\"goldfinal\"] == \"E\":\n",
        "        gold_final_interpretation = \"unsure\"\n",
        "        count_E += 1\n",
        "    elif trace[\"goldfinal\"] == \"T\":\n",
        "        gold_final_interpretation = \"traceable\"\n",
        "        count_T += 1\n",
        "    else:  # Assuming the only other value is \"N\"\n",
        "        gold_final_interpretation = \"not traceable\"\n",
        "        count_N += 1\n",
        "\n",
        "    # Create an entry for the JSONL content\n",
        "    jsonl_entry = {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a tracelink identifying system between a requirement and method details.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Check if the following requirement: {trace['requirement']} is linked with method name: {trace['methodname']} and method implementation: {method_implementation}\"},\n",
        "            {\"role\": \"assistant\", \"content\": gold_final_interpretation}\n",
        "        ]\n",
        "    }\n",
        "    jsonl_content.append(jsonl_entry)\n",
        "\n",
        "# Print the counts for each goldfinal value\n",
        "print(f'Count of T (traceable): {count_T}')\n",
        "print(f'Count of E (unsure): {count_E}')\n",
        "print(f'Count of N (not traceable): {count_N}')\n",
        "print(f'Count of Total Traces): {count_T + count_N + count_E}')\n",
        "# Convert the list to JSONL string (if needed for further processing)\n",
        "jsonl_str = \"\\n\".join(json.dumps(entry) for entry in jsonl_content)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jsonl_file_path = '/content/total_traces_links.jsonl'\n",
        "\n",
        "# Write the JSONL string to the file\n",
        "with open(jsonl_file_path, 'w') as file:\n",
        "    file.write(jsonl_str)\n",
        "\n",
        "# Confirm the path to the saved JSONL file\n",
        "jsonl_file_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9zAUQ1Bh4bLU",
        "outputId": "61c368a1-4d29-42de-b896-4580114c8855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/total_traces_links.jsonl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Binary Pre-Processing**"
      ],
      "metadata": {
        "id": "yBdl0ymQ9vJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "# Function to sample a specific number of traces based on user input\n",
        "def sample_specific_traces(traces, train_count, validation_count, test_count):\n",
        "    random.shuffle(traces)\n",
        "    train_traces = traces[:train_count]\n",
        "    remaining_traces = traces[train_count:]\n",
        "    validation_traces = remaining_traces[:validation_count]\n",
        "    test_traces = remaining_traces[validation_count:validation_count + test_count]\n",
        "    return train_traces, validation_traces, test_traces\n",
        "\n",
        "# Load the traces.json and methods.json files\n",
        "with open('/content/traces.json', 'r') as file:\n",
        "    traces_data = json.load(file)\n",
        "with open('methods.json', 'r') as file:\n",
        "    methods_data = json.load(file)\n",
        "\n",
        "# Separate traces based on goldfinal value, excluding traces with goldfinal value \"E\"\n",
        "traces_T = [trace for trace in traces_data if trace[\"goldfinal\"] == \"T\"]\n",
        "traces_N = [trace for trace in traces_data if trace[\"goldfinal\"] == \"N\"]\n",
        "\n",
        "# Interactive input for the sizes of the training, validation, and testing sets and their compositions\n",
        "training_set_size = int(input(\"Enter the desired size for the training set: \"))\n",
        "validation_set_size = int(input(\"Enter the desired size for the validation set: \"))\n",
        "testing_set_size = int(input(\"Enter the desired size for the testing set: \"))\n",
        "\n",
        "# Input for the number of 'T' and 'N' traces in the training, validation, and testing sets\n",
        "training_T_count = int(input(\"Enter the number of 'T' traces for the training set: \"))\n",
        "training_N_count = training_set_size - training_T_count\n",
        "validation_T_count = int(input(\"Enter the number of 'T' traces for the validation set: \"))\n",
        "validation_N_count = validation_set_size - validation_T_count\n",
        "testing_T_count = int(input(\"Enter the number of 'T' traces for the testing set: \"))\n",
        "testing_N_count = testing_set_size - testing_T_count\n",
        "\n",
        "# Sample the specified numbers of 'T' and 'N' traces for training, validation, and testing sets\n",
        "train_T, validation_T, test_T = sample_specific_traces(traces_T, training_T_count, validation_T_count, testing_T_count)\n",
        "train_N, validation_N, test_N = sample_specific_traces(traces_N, training_N_count, validation_N_count, testing_N_count)\n",
        "\n",
        "# Combine and shuffle the training, validation, and testing sets\n",
        "training_data = train_T + train_N\n",
        "validation_data = validation_T + validation_N\n",
        "testing_data = test_T + test_N\n",
        "random.shuffle(training_data)\n",
        "random.shuffle(validation_data)\n",
        "random.shuffle(testing_data)\n",
        "\n",
        "# Function to create JSONL content from traces (remains unchanged)\n",
        "def create_jsonl_content(traces):\n",
        "    jsonl_content = []\n",
        "    for trace in traces:\n",
        "        method_implementation = None\n",
        "        for method in methods_data:\n",
        "            if method.get(\"id\") == trace[\"methodid\"]:\n",
        "                method_implementation = method.get(\"method\")\n",
        "                break\n",
        "        gold_final_interpretation = \"True\" if trace[\"goldfinal\"] == \"T\" else \"False\" if trace[\"goldfinal\"] == \"N\" else \"unsure\"\n",
        "        jsonl_entry = {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": \"You are a traceability identifying system. Your task is to analyze if a given requirement is linked with the specified method name and its implementation.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Check if the following REQUIREMENT: '{trace['requirement']}' is linked with METHOD NAME: '{trace['methodname']}' and METHOD IMPLEMENTATION: '{method_implementation}'. After analysis, only state whether the requirement is linked ('True') or not linked ('False')\"},\n",
        "                {\"role\": \"assistant\", \"content\": gold_final_interpretation}\n",
        "            ]\n",
        "        }\n",
        "        jsonl_content.append(jsonl_entry)\n",
        "    return jsonl_content\n",
        "\n",
        "# Create JSONL content for training, validation, and testing datasets\n",
        "training_jsonl_content = create_jsonl_content(training_data)\n",
        "validation_jsonl_content = create_jsonl_content(validation_data)\n",
        "testing_jsonl_content = create_jsonl_content(testing_data)\n",
        "\n",
        "# Print sizes and counts of the created datasets\n",
        "print(f\"Training set size: {len(training_data)}\")\n",
        "print(f\"Validation set size: {len(validation_data)}\")\n",
        "print(f\"Testing set size: {len(testing_data)}\")\n",
        "print(f\"Training Dataset Counts: {{'T': {len(train_T)}, 'N': {len(train_N)}}}\")\n",
        "print(f\"Validation Dataset Counts: {{'T': {len(validation_T)}, 'N': {len(validation_N)}}}\")\n",
        "print(f\"Testing Dataset Counts: {{'T': {len(test_T)}, 'N': {len(test_N)}}}\")\n",
        "\n",
        "#From the traces.json (total)\n",
        "#\"goldfinal\":\"N\" 26,842\n",
        "#\"goldfinal\":\"T\" 1429\n",
        "# T is 13.7% of (T+N)\n",
        "\n",
        "# 5000 total for training\n",
        "# 800 for T in traning\n",
        "\n",
        "# 1000 total for validation\n",
        "# 150 for T in validation\n",
        "\n",
        "# 1000 total for testing\n",
        "# 150 T in Testing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNACTcdPhmVz",
        "outputId": "0f90d80a-7807-42f7-8f05-5e5649346c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the desired size for the training set: 1000\n",
            "Enter the desired size for the validation set: 10\n",
            "Enter the desired size for the testing set: 1000\n",
            "Enter the number of 'T' traces for the training set: 150\n",
            "Enter the number of 'T' traces for the validation set: 5\n",
            "Enter the number of 'T' traces for the testing set: 150\n",
            "Training set size: 1000\n",
            "Validation set size: 10\n",
            "Testing set size: 1000\n",
            "Training Dataset Counts: {'T': 150, 'N': 850}\n",
            "Validation Dataset Counts: {'T': 5, 'N': 5}\n",
            "Testing Dataset Counts: {'T': 150, 'N': 850}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the JSONL\n"
      ],
      "metadata": {
        "id": "5GL9gRum4oFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print sizes of the created datasets\n",
        "print(f\"Training set size: {len(training_data)}\")\n",
        "print(f\"Validation set size: {len(validation_data)}\")\n",
        "print(f\"Testing set size: {len(testing_data)}\")\n",
        "print(f\"Training Dataset Counts: {{'T': {len(train_T)}, 'N': {len(train_N)}}}\")\n",
        "print(f\"Validation Dataset Counts: {{'T': {len(validation_T)}, 'N': {len(validation_N)}}}\")\n",
        "print(f\"Testing Dataset Counts: {{'T': {len(test_T)}, 'N': {len(test_N)}}}\")\n",
        "\n",
        "# Function to save JSONL content to a file with dataset size in the filename\n",
        "def save_jsonl_content(filename, size, content):\n",
        "    # Attach the dataset size to the file name\n",
        "    filename_with_size = f\"{filename.split('.')[0]}_{size}.jsonl\"\n",
        "    with open(filename_with_size, 'w', encoding='utf-8') as f:\n",
        "        for entry in content:\n",
        "            json_line = json.dumps(entry) + \"\\n\"  # Convert dict to JSON string and add newline\n",
        "            f.write(json_line)\n",
        "\n",
        "# Save training, validation, and testing datasets to JSONL files with dataset sizes in filenames\n",
        "save_jsonl_content('GPT4_training_dataset_binary_JHOT', len(training_data), training_jsonl_content)\n",
        "save_jsonl_content('GPT4_validation_dataset_binary_JHOT', len(validation_data), validation_jsonl_content)\n",
        "save_jsonl_content('GPT4_testing_dataset_binary_JHOT', len(testing_data), testing_jsonl_content)\n",
        "\n",
        "print(\"Training, validation, and testing datasets have been saved with sizes in the filenames.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shzIXOlkiikA",
        "outputId": "b7f81a3e-2b00-453c-a5c7-7982caacc89a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 1000\n",
            "Validation set size: 10\n",
            "Testing set size: 1000\n",
            "Training Dataset Counts: {'T': 150, 'N': 850}\n",
            "Validation Dataset Counts: {'T': 5, 'N': 5}\n",
            "Testing Dataset Counts: {'T': 150, 'N': 850}\n",
            "Training, validation, and testing datasets have been saved with sizes in the filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Path to the uploaded file\n",
        "file_path = '/content/testing_dataset_binary_1000.jsonl'\n",
        "\n",
        "# Initialize lists to store data\n",
        "ids = []\n",
        "inputs = []\n",
        "traceability = []\n",
        "\n",
        "# Read the file and extract required information\n",
        "with open(file_path, 'r') as file:\n",
        "    for i, line in enumerate(file):\n",
        "        data = json.loads(line)\n",
        "        user_content = \"\"\n",
        "        assistant_content = \"\"\n",
        "        for message in data['messages']:\n",
        "            if message['role'] == 'user':\n",
        "                user_content = message['content']\n",
        "            elif message['role'] == 'assistant':\n",
        "                assistant_content = message['content']\n",
        "\n",
        "        # Populate lists\n",
        "        ids.append(i+1)\n",
        "        inputs.append(user_content)\n",
        "        traceability.append(True if assistant_content == \"True\" else False)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({'ID': ids, 'input': inputs, 'Traceability': traceability})\n",
        "\n",
        "# Define the output CSV file path\n",
        "output_csv_path = 'Final_Metrics.csv'\n",
        "\n",
        "# Save the DataFrame to CSV\n",
        "df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "# Return the path of the created CSV file\n",
        "output_csv_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5PPGrbKiGyWu",
        "outputId": "6afd6ebd-2729-4796-c3c5-a941b78a1b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Final_Metrics.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ]
}